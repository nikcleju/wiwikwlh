<!--
<p class="center logo">
<h1>Information Theory 2015-2016</h1> ![](img/title.png)
</p>
-->


Information Theory 2015-2016
======

Nicolae Cleju <!--(<a class="author" href="https://twitter.com/smdiehl">@smdiehl</a> )-->

Faculty of Electronics, Telecommunications and Information Technology,

The "Gheorghe Asachi" Technical University of Iasi.

This page contains all the materials and information for the course of Information
Theory taking place during second semester 2015-2016.

Feel free to contact me for any questions, problems or comments regarding this material
or anything else discussed in class.

The template of this webpage is by [Stephen Diehl](http://dev.stephendiehl.com/hask/).


Introduction
======

The teaching activities are performed by:

* Lectures: Nicolae Cleju
* Laboratory: Daniel Matasaru

Office hours: TBD

The final grade for the class is computed as follows:

    final grade = 0.75 exam grade + 0.25 lab grade

The teaching activities consist of:

* 14 weeks of lectures (3h each)
* 14 weeks of laboratories (2h each)

Course structure
======
1. Chapter I:   Discrete Information Sources
2. Chapter II:  Discrete Transmission Channels
3. Chapter III: Source Coding
4. Chapter IV:  Channel Coding 

Bibliography
======

1. ***Elements of Information Theory*, Valeriu Munteanu, Daniela Tarniceriu, Ed. CERMI 2007**
1. *Elements of Information Theory*, Thomas M. Cover, Joy A. Thomas, 2nd Edition, Wiley 2006
1. *Information and Coding Theory*, Gareth A. Jones, J. Mary Jones, Springer 2000
1. *Transmisia si codarea informatiei*, lectures at ETTI (Romanian)

# Slides download [pdf]
Download [here](slides.pdf)

# Laboratory
1. Linear Filters ([pdf](labs/L01_IT_LinearFilters.pdf))
2. Pseudo-random Binary Sequences Generator ([pdf](labs/L02_IT_PseudorandomBinarySequenceGenerator.pdf))
3. Variable Entropy Generator ([pdf](labs/L03_IT_VariableEntropyGenerator.pdf))

# Exam preparation

The exam will consist of 4 subjects: 2 practical exercises and 2 theoretical subjects.

In week 8 there will be an optional partial exam (attendance is optional), from the first two chapters only: 

1. Chapter I: Discrete information sources 
2. Chapter II: Discrete Transmission Channels

Those who pass the partial exam, will have a final exam only from the final two chapters (III and IV).
 Possibly shorter (3 problems only? not sure).
 
Those who do not attend or do not pass the partial exam, will take a full exam at the end, as usual.

**Practical exercises:** will be very similar to the problems solved at the seminars and the examples given 
during the lectures. I cannot give you any other example.

**Theoretical subjects (condensed list):**

* Chapter I: Discrete Information Sources
    1. Definitions of: discrete memoryless source, information of a particular event, entropy of a memoryless source. Examples.
    2. Properties of entropy, with proofs (3 properties: non-negative, maximum, diversification).The proof of the second property only for the particular case of a source with two messages (as we did in class)
    3. Entropy of a N-th order extension, with proof only for the particular case of 2nd order extension (as we did in class)
    4. Definition of: source with memory m, state of a source with memory, transition, transition matrix, ergodic source.

* Chapter II: Discrete Transmission Channels
    1. Definition of a discrete transmission channel. The joint probability matrix, the channel matrix, and the relation between them.
    2. Mutual information: definition, equation, properties (three properties, without proofs, just stated)
    3. Particular types of communication channels: zero equivocation, zero average error, uniform with respect to the input, uniform with respect to the output, symmetric
    4. Definition of channel capacity. Efficiency and redundancy of a channel
    5. Capacity of a binary symmetric channel (with proof)

* Chapter III: Source coding
    1. Definition of non-singular, uniquely decodable and instantaneous code. Relation between them. Examples.
    2. Kraft inequality theorem (with proof).
    3. Average length of a Shannon code (with proof).
    4. Shannon's first theorem (with proof on slides).

* Chapter IV:
    1. Definitions: error correcting code, block code, linear code, coding rate, t-error-detecting code, t-error-correcting code, systematic code, cyclic code
    2. Definition of Hamming distance and minimum Hamming distance of a code. Number of errors that can be detected or corrected depending on dHmin (with proof)
    3. Conditions on matrix [H] for error detection and correction
    4. Hamming codes: definition, example of matrix [H], properties

*Note*: "No proof" means that you should only state the theorem / definition / property, without any proof. "With proof" means I expect the proof to be included.

# Exam results

The final grades following the exam on 13.06.2016 are [here](grades/NoteFinaleTCI_2015_2016_password.pdf).
The password is your group number. 

The final grades following the re-exam (restanta) on 02.07.2016 are [here](grades/RestantaIT_2016_07_02_pwd.pdf).
The password is your group number. 

The final grades following the re-exam (restanta) on 05.09.2016 are [here](grades/RestantaIT_2016_09_05_pwd.pdf).
The password is your group number. 

The final grades following the re-examination on 16.09.2016 are [here](grades/ReexaminareIT_2016_09_16_pwd.pdf).
The password is your group number. 

